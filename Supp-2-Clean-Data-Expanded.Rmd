---
title: "Basic Data Loading and Transformation - Extended"
author: "Angela Zoss"
date: "`r Sys.Date()`"
output: html_document
---

```{r}

# Run the first time you start working with this file

install.packages("tidyverse")
install.packages("readxl")


```


```{r}

# Loading required packages

library(tidyverse)
library(readxl)

```

## Loading data

The `tidyverse` comes with some built-in data loading functions that I recommend
over the ones that are included in base R. The ones I use most commonly are `read_csv`
and `read_xlsx`, which is actually in a separate package called `readxl`. 

Both of these functions can load data either from a local data file or from something
shared on the web. Let's grab the dataset from the Open Refine lesson for Library Carpentries.

```{r}

lc_open_refine <- read_csv(
  'https://github.com/LibraryCarpentry/lc-open-refine/raw/gh-pages/data/doaj-article-sample.csv',
  col_types = cols(.default = "c", Date = col_date(format="%m/%d/%Y")) 
  )

```

The `read_csv()` function does a few things automatically. It assumes that the first 
row of the dataset contained the column names. It can also each column to try to guess 
the data type. Usually the tidyverse data reading functions do a great job of guessing
data types, but you can also specify them manually if you need to. Here, I specified
that Date is a date but all others are character variables.

We spent a long time talking about data types and accessing data. Now that we have
a dataset and are learning how to use the pipe, I want to show a few handy functions 
for accessing data in a data frame.

## `select()` for isolating columns

Instead of using `$` or `[]` or `[[]]` to grab one of the columns in the data frame,
let's use our words.

The `select()` function has a wide variety of ways to specify the columns you want.
Let's start with names. To select by name, all you have to do is list all of the
column names you want, separated by commas. The first argument in the function
will always be the name of the data frame.

```{r}

select(lc_open_refine, Title, Authors)

```

I don't know about you, but I prefer that to having to remember the punctuation marks.

What else can you do with `select()`? You can still use `-` to exclude columns, but now
you can do it by name.

```{r}

select(lc_open_refine, -Subjects)

```


```{r}

select(lc_open_refine, -Language, -Citation)

```

What if you have a group of columns that all have similar names and you want to
include or exclude all of them? You can use a helper function called `starts_with()`.

```{r}

select(lc_open_refine, starts_with('L'))

select(lc_open_refine, -starts_with('L'))

```

### `select()` Helper Functions

#### String search functions

* starts_with(): look for a substring at the beginning of the column name
* contains(): look for a substring anywhere in the column name
* ends_with(): look for a substring at the end of the column name
* matches(): look for a regular expression pattern in the column name
* num_range(): look for a set of columns that contains the specified number range

#### Set functions

* all_of(): throws error if any columns in list are missing; good if you have a pre-defined
list of column names you're expecting
* any_of(): returns any in list that are there, ignores missing ones

#### Position functions
* everything(): select everything; can be used to reorder variables
* last_col(): selects just the last column, but can also use an offset to count in from the end

#### Make your own test

* where(): use any function that returns true or false for each column vector (the values, not the name)

```{r}

select(lc_open_refine, DOI, everything())
select(lc_open_refine, where(is.numeric))

```

You can mix and match these kinds of options, too.

```{r}

select(lc_open_refine, starts_with('L'), DOI)

select(lc_open_refine, starts_with('L'), DOI, everything(), -URL)

```

You can even use some punctuation, but more flexibly than in base R. For example, 
the colon `:` still specifies a range, but you can say a range of names, and you
don't need a `c()`.

```{r}

select(lc_open_refine, Title:Subjects)

```

See the `select()` help documentation for more tips and examples.

Back to our data set. Try selecting the DOI column.

```{r}

select(lc_open_refine, DOI)

```

Now, let's pretend we need to remove any rows where the DOI is empty.


## `filter()` for isolating rows

In `tidyverse`, the function for retrieving a set of rows is `filter()`. Usually, 
if you're limiting a dataset to a subset of rows, you want to do it on some specific
criteria, usually related to the data.

Remember the syntax we used for vector subsets?

`sample_vector[sample_vector > 4]`

We're back to something like that. This time, we're going to specify the name of
the data frame, and then we're going to refer to the column and build the test, all
inside the `filter()` function. The general pattern is:

`filter(data_frame, some test for the rows)`

This is a bit trickier because all of our variables
are text variables and we haven't learned much that we can do with them yet. Luckily,
our Date column can be treated like a number. We can also test for simple equivalence,
like rows where the language is (or is not) "EN".

```{r}

filter(lc_open_refine, Date < as.Date("2015-01-02"))

filter(lc_open_refine, Language != "EN")

```

To remove empty DOIs, we need to know how to test if values are null. In R, that function is:

`is.na(name_of_column)`

Combine this with `filter()` to find the NA values in the DOI column:

```{r, echo=TRUE}

filter(lc_open_refine, is.na(DOI))

```

But we want to remove the NAs, not all of the other rows. How do we get the opposite?

We can use `!` to get the negative of a test.

`!is.na(name_of_column)`

Or, in this case:

```{r, echo=TRUE}

filter(lc_open_refine, !is.na(DOI))

```

The tidyverse actually has a handy function that does this: drop_na()

```{r, echo=TRUE}

drop_na(lc_open_refine, DOI)

```

## Fix inconsistencies in Language

When we're exploring a new data set, we might want to ask questions like: What are the unique values of a column? How many records are there for each value?

Tidyverse also has a handy function for this: `count()`.

`count(data_frame, name_of_column)`

Let's use `count()` to summarize the Language column.

```{r, echo=TRUE}

count(lc_open_refine, Language)

```

This shows an inconsistency in some of the records. We need to edit the data
values to standardize them.

### How to edit a variable: mutate

The `mutate()` function allows you to make changes to an existing variable or to
create a new variable. The general pattern is:

`mutate(data_frame, name_of_column = new value or expression)`

We know we need to mutate the Language column, but we have to come up with an 
expression that will do the right mutation. What should the expression do?

* If the Language is "English", change to "EN"
* Otherwise, leave the original value

There are multiple ways we could accomplish this in R. Here are a couple of options:

* Use a simple if/then statement to look for "English" and replace it when we find
* Use a simple string replace to look for "English" and replace it when we find

### Option 1: If/then

R has a more complicated way of writing if-then statements. The `ifelse()` function 
is a simple version that works really well with the mutate function. The general 
pattern is:

`ifelse(test, what to do if true, what to do if false)`

So, in this case, we will want a statement like this:

`ifelse(Language == "English", "EN", Language)`

### Option 2: String replacement

String replacement is similar, but it's more flexible. The function `str_replace()`
will look for English, even if it's not the full contents of the cell in the table.

`str_replace(text, what to replace, what to use instead)`

In our case, we'll want a statement like this:

`str_replace(Language, "English", "EN")`

Now that we have our expression, we need to plug it into the `mutate()` function.

```{r, echo=TRUE}

mutate(lc_open_refine, Language = ifelse(Language == "English", "EN", Language))

```

Or, if we're using option 2:

```{r, echo=TRUE}

mutate(lc_open_refine, Language = str_replace(Language, "English", "EN"))

```

## Fix inconsistencies in Publisher

Next let's check out the Publisher column. Try using `count()` to explore the categories.

```{r, echo=TRUE}

count(lc_open_refine, Publisher)

```

What needs to be fixed? Can you think of a string replacement that will work?


```{r, echo=TRUE}

mutate(lc_open_refine, Publisher = str_replace(Publisher, "", ""))

```


## Chaining operations

Almost every data science toolkit is going to have a way of chaining together 
different kinds of data operations to form a longer sequence. Basically, this saves
you from having to store the results of each operation in a separate variable.

For example:

```{r}

days_per_year <- 365
hours_per_year <- days_per_year * 24
minutes_per_year <- hours_per_year * 60

minutes_per_year

```

With mathematical operations, we can just write them in a long line.

```{r}

minutes_per_year <- 365 * 24 * 60

minutes_per_year

```

That's where we want to get with datasets, too. But we have to learn one tool first -
the syntax for chaining functions together in R (or, at least, in `tidyverse` packages).

### The magrittr pipe

`%>%`

That's the syntax. It's called a pipe, and it was originally developed for a package
called magrittr. (Many R packages end with a single r, rather than "er.")

The pipe works by taking whatever the result of the previous command is and injecting
or piping it straight into the next command. Let's start with a simple example.

Here is a basic function that returns the class of a data element in R: `class()`.
Inside the parentheses, you would normally type the variable name or the data element
you want to find the class of. If you want to use the pipe, though, you could start
with the data element and then pipe it to the `class()` function.

```{r}

class("a")

"a" %>% class()

```

Those two statements are identical. When you use the pipe, whatever was on the left
side of the pipe gets inserted into the parentheses of the function on the right.

We haven't learned many functions yet, but let me introduce another very useful one.
It is the `seq()` function, and it can be used to create a vector of values based on 
a sequence. For example, you could create a sequence of all of the even numbers up
to 100, or count by fives or 10s.

To find the help page for a function, you can use a question mark `?` followed by the
function name.

```{r}

?seq

```

Let's use a sequence from 0 to 100, counting by 10s.

```{r}

seq(from = 0, to = 100, by = 10)

```

Now, let's say I want to learn the class of that sequence. Let's try it the long
way.

```{r}

tens <- seq(from = 0, to = 100, by = 10)
class(tens)

```

Now, using the pipe.

```{r}

seq(from = 0, to = 100, by = 10) %>% class()

```

Is it bad to store things as variables? Not at all! But sometimes it's just not
necessary and can interrupt the flow of a set of data steps. Also, the more
variables you're storing, the harder it might be to keep track of them, and you might
accidentally end up using the wrong one at some point.

I usually store or print results temporary while I'm building a set of steps. When 
I'm confident the steps make sense and are working the way I want, I will try to
simplify them into a chain that makes sense. Of course, it's always good to be testing
the results of any data science process to make sure your results are what you expect.

Piping vectors is all well and good, but ultimately our goal is to use pipes to help
us work with tabular data more naturally. 

## Combine previous data transformations

Let's create a single chain of data transformations to remove null DOIs and fix the Language and Publisher columns.

First, let's take the data set and pipe it to a function that will drop the NAs in 
the DOI column.

```{r, echo=TRUE}

lc_open_refine %>% drop_na(DOI)

```

Next, add another step to change "English" to "EN" in Language.

```{r, echo=TRUE}

lc_open_refine %>% drop_na(DOI) %>% mutate(Language = str_replace(Language, "English", "EN"))

```

To avoid long lines, you can start a new line after a pipe.

```{r, echo=TRUE}

lc_open_refine %>% 
  drop_na(DOI) %>% 
  mutate(Language = str_replace(Language, "English", "EN"))

```

Finally, add a link that will fix the Publisher space problem.

```{r, echo=TRUE}

lc_open_refine %>% 
  drop_na(DOI) %>% 
  mutate(Language = str_replace(Language, "English", "EN")) %>%
  mutate(Publisher = str_replace(Publisher, "  ", " "))

```

The pipeline is working, but the transformed data aren't getting saved anywhere,
they're just getting printed. Let's pick a new name and save the whole pipeline
to that new variable.

```{r, echo=TRUE}

lc_open_refine_transformed <- lc_open_refine %>% 
  drop_na(DOI) %>% 
  mutate(Language = str_replace(Language, "English", "EN")) %>%
  mutate(Publisher = str_replace(Publisher, "  ", " "))

```


## Advanced: Split apart and clean Subjects, then re-join

Inspect Subjects:

```{r, echo=TRUE}

lc_open_refine %>% select(Subjects)

```

Plan for data transformation: 

* Split Subjects using the "|" character
* Rearrange data so each subject is a separate row in the same column
* Find and fix slight variations
* Recombine the subjects into a pipe-separated list

Bfore we get started, this dataset contains a duplicate row that will start causing
problems. Let's remove it with the `distinct()` function.


```{r}

lc_open_refine %>% distinct()

```


Now, let's split subjects into multiple columns:

General pattern: 
`separate_wider_delim(data, column, delimiter, names_sep="separator")`

```{r, echo=TRUE}

lc_open_refine %>% 
  distinct() %>%
  separate_wider_delim(Subjects, "|", names_sep="-", too_few = "align_start")

```

Now, we need to add a step to rearrange the data to change the columns into
rows so we have one subject per row. In the tidyverse, this is called a "pivot."

There are a few pivot options. In this case, we want to pivot from a wider data set
(lots of columns) to a longer data set (lots of rows). The function is: `pivot_longer()`,
and the general pattern is:

`pivot_longer(data, columns)`

What's great about this function is that we can use all of the same helper functions from
`select()` to identify the columns we want to pivot. Here, we want to pivot all of the 
columns we created with the `separate...` command, and all of those start with "Subject."
So let's use a `starts_with()` helper function to make sure we're getting all the right
columns.

```{r}

lc_open_refine %>% 
  distinct() %>%
  separate_wider_delim(Subjects, "|", names_sep="-", too_few = "align_start") %>%
  pivot_longer(starts_with("Subject"))

```

The pivot process results in two columns - one that has the original column name
and the other has the value from the cell in the table. 

The `separate...()` command created 19 columns, but not every record had 19 subjects,
so our table has a lot of NA values now. Let's drop NAs in the value column.

```{r}

lc_open_refine %>% 
  distinct() %>%
  separate_wider_delim(Subjects, "|", names_sep="-", too_few = "align_start") %>%
  pivot_longer(starts_with("Subject")) %>%
  drop_na(value)

```

Cleaning the actual values of this column is a bit out of scope for right now. Let's 
pretend we did something with those here. Now it's time to combine those values
back into a list.

First, we'll reverse the pivot by using `pivot_wider()`.

```{r}

lc_open_refine %>% 
  distinct() %>%
  separate_wider_delim(Subjects, "|", names_sep="-", too_few = "align_start") %>%
  pivot_longer(starts_with("Subject")) %>%
  pivot_wider()

```

Now we're back to 1000 rows! But we still have to combine all the subject columns back
together. The function for that is `unite()`. Again, we can use `starts_with()` to select
the Subjects columns. The general pattern is:

`unite(data, "name_of_new_column", columns to combine)`

One useful thing to add is "na.rm = TRUE". Any columns where the subject is empty will
just be ignored.

```{r}

lc_open_refine %>% 
  distinct() %>%
  separate_wider_delim(Subjects, "|", names_sep="-", too_few = "align_start") %>%
  pivot_longer(starts_with("Subject")) %>%
  pivot_wider() %>%
  unite("Subjects", starts_with("Subject"), na.rm=TRUE)

```

